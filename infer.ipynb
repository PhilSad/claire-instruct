{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNc2a6DH6NdjOdE8zBg4qcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilSad/claire-instruct/blob/main/infer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyN0z6FTkpzm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_name = \"PhilSad/Claire-7b-0.1-instruct\"\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"OpenLLM-France/Claire-7B-0.1\")\n",
        "model_instruct = transformers.AutoModelForCausalLM.from_pretrained(model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "pipeline_instruct = transformers.pipeline(\"text-generation\", model=model_instruct, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "a6DGUbIFkuyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_instruct = transformers.pipeline(\"text-generation\", model=model_instruct, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "vsmbP0bzlFhr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_kwargs = dict(\n",
        "    num_return_sequences=1,                    # Number of variants to generate.\n",
        "    return_full_text= False,                   # Do not include the prompt in the generated text.\n",
        "    max_new_tokens=200,                        # Maximum length for the output text.\n",
        "    do_sample=True, top_k=10, temperature=1.0, # Sampling parameters.\n",
        "    pad_token_id=tokenizer.eos_token_id,       # Just to avoid a harmless warning.\n",
        ")\n",
        "\n",
        "prompt = \"\"\"Ci-dessous se trouve une instruction qui décrit une tâche. Écrivez une réponse qui complète de manière appropriée la demande.\n",
        "\n",
        "### Instruction :\n",
        "Donne moi la recette pour faire un bon mojito\n",
        "\n",
        "### Réponse :\"\"\"\n",
        "\n",
        "completions = pipeline_instruct(prompt, **generation_kwargs)\n",
        "for completion in completions:\n",
        "    print(prompt + \" […]\" + completion['generated_text'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oz_wv61k7qd",
        "outputId": "1c0989a1-8351-40ec-a8a5-04127cb6664a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ci-dessous se trouve une instruction qui décrit une tâche. Écrivez une réponse qui complète de manière appropriée la demande.\n",
            "\n",
            "### Instruction :\n",
            "Donne moi la recette pour faire un bon mojito\n",
            "\n",
            "### Réponse : […]\n",
            "Le Mojito est un cocktail à base de rhum, de citron vert et de sirop de sucre. Il peut être consommé froid ou chaud. Pour préparer un bon mojito, commencez avec un verre à cocktail. Remplissez le verre aux trois quarts avec de la glace et ajoutez deux à quatre cuillères à soupe de sucre. Mélangez jusqu'à ce que le sucre ait totalement fondu. Faites ensuite infuser une tranche de citron vert dans le verre. Versez le rhum, le club soda, et complétez avec de la glace. Mélangez bien et c'est prêt!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TexstFjClBvg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}