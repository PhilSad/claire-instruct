{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.27.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Claire-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fa63640afa4b3e92443c07727092c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_name = \"OpenLLM-France/Claire-7B-0.1\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model_base = transformers.AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True                          # For efficient inference, if supported by the GPU card\n",
    ")\n",
    "\n",
    "pipeline_base = transformers.pipeline(\"text-generation\", model=model_base, tokenizer=tokenizer)\n",
    "generation_kwargs = dict(\n",
    "    num_return_sequences=1,                    # Number of variants to generate.\n",
    "    return_full_text= False,                   # Do not include the prompt in the generated text.\n",
    "    max_new_tokens=200,                        # Maximum length for the output text.\n",
    "    do_sample=True, top_k=10, temperature=1.0, # Sampling parameters.\n",
    "    pad_token_id=tokenizer.eos_token_id,       # Just to avoid a harmless warning.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Donne moi la recette pour faire un bon mojito- […]trempe-salé\n",
      "- C'est une recette de famille\n",
      "- Ah\n",
      "- Tu peux la faire en trente secondes Mais je te préviens c'est une recette à risque\n",
      "- Je ne risque rien\n",
      "- Il y a de l'alcool\n",
      "- Tu peux faire avec du jus d'orange\n",
      "- Je vais pas faire ça\n",
      "- Non non\n",
      "- Mais il faut que tu fasses quelque chose\n",
      "- Tu peux faire avec des fruits\n",
      "- Tu peux faire avec du citron\n",
      "- Tu peux faire avec du citron\n",
      "- C'est une recette de famille\n",
      "- Ah bon C'est une recette de famille Mais il y a de l'alcool C'était quoi déjà le truc\n",
      "- Non il faut que tu fasses quelque chose C'est une recette de famille\n",
      "- C'est une recette de famille\n",
      "- Mais je te préviens il y a de l'alcool\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"- Donne moi la recette pour faire un bon mojito\\\n",
    "-\"\"\"\n",
    "completions = pipeline_base(prompt, **generation_kwargs)\n",
    "for completion in completions:\n",
    "    print(prompt + \" […]\" + completion['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claire-7b-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"OpenLLM-France/Claire-7B-0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f607a8d649b74d2bb4e19b6954f5b3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2712b6189d6c48c2aa95338aaf454bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/113 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"PhilSad/Claire-7b-0.1-instruct\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"OpenLLM-France/Claire-7B-0.1\")\n",
    "model_instruct = transformers.AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instruct = transformers.pipeline(\"text-generation\", model=model_instruct, tokenizer=tokenizer)\n",
    "generation_kwargs = dict(\n",
    "    num_return_sequences=1,                    # Number of variants to generate.\n",
    "    return_full_text= False,                   # Do not include the prompt in the generated text.\n",
    "    max_new_tokens=200,                        # Maximum length for the output text.\n",
    "    do_sample=True, top_k=10, temperature=1.0, # Sampling parameters.\n",
    "    pad_token_id=tokenizer.eos_token_id,       # Just to avoid a harmless warning.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ci-dessous se trouve une instruction qui décrit une tâche. Écrivez une réponse qui complète de manière appropriée la demande.\n",
      "\n",
      "### Instruction :\n",
      "Donne moi la recette pour faire un bon mojito\n",
      "\n",
      "### Réponse : […]\n",
      "Le mojito est un cocktail alcoolisé originaire des Antilles, et c'est maintenant l'un des cocktails les plus appréciés. Les ingrédients du mojito comprennent du citron vert, du sirop de sucre, de l'eau gazeuse, de la menthe fraîche et de l'épeautre. La première étape pour faire un bon cocktail Mojito est de couper le citron vert en fines rondelles avec un couteau. Vous devez ensuite frotter le bord de votre verre avec le citron vert, et le mettre avec vos autres ingrédients. Vous devez ensuite verser de l'eau gazeuse dans le verre, et le faire glisser. Pour finir, vous devez mettre les feuilles de menthe fraîche et les bâtonnets de sucre dans le verre, et servir la boisson à vos invités! Bonne chance et bonne dégust\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"Ci-dessous se trouve une instruction qui décrit une tâche. Écrivez une réponse qui complète de manière appropriée la demande.\n",
    "\n",
    "### Instruction :\n",
    "Donne moi la recette pour faire un bon mojito\n",
    "\n",
    "### Réponse :\"\"\"\n",
    "completions = pipeline_instruct(prompt, **generation_kwargs)\n",
    "for completion in completions:\n",
    "    print(prompt + \" […]\" + completion['generated_text'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
